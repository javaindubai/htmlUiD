from langchain_community.chat_models import ChatOllama
# ollama = ChatOllama(base_url='http://localhost:11434', model="mistral")
# print(ollama("why is the sky blue"))

ollama = ChatOllama(
    temperature=0,
    base_url='http://localhost:11434',
    model="mistral",
    streaming=True,
    # seed=2,
    top_k=10,  # A higher value (100) will give more diverse answers, while a lower value (10) will be more conservative.
    top_p=0.3,  # Higher value (0.95) will lead to more diverse text, while a lower value (0.5) will generate more focused text.
    num_ctx=3072,  # Sets the size of the context window used to generate the next token.
)
print(ollama("why is the sky blue"))